---
title: "Cliam Fraud"
author: "BonnyFace Kalong"
date: "`r Sys.Date()`"
output: pData_document
---

```{r}
Data<-read.csv("./DataSet/fraud_oracle.csv")
y <- Data$FraudFound_P
head(Data)
```
```{r}
library(visdat)
vis_dat(Data)
```

```{r}
library(skimr)
skimmed <- skim_to_wide(Data)
skimmed[, c(1:5, 9:11, 13, 15:16)]
```
```{r}
```

```{r,warning=FALSE}
library(ggplot2)
library(gridExtra)

# Create a list of countplots
plots <- list(
  ggplot(Data, aes(x=Month, fill=FraudFound_P)) + 
    geom_bar(position="dodge") + 
    scale_x_discrete(limits=c('Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec')) + 
    labs(title="Month"),
  
  ggplot(Data, aes(x=DayOfWeek, fill=FraudFound_P)) + 
    geom_bar(position="dodge") + 
    scale_x_discrete(limits=c('Sunday','Monday','Tuesday','Wednesday','Thursday','Friday','Saturday')) + 
    labs(title="Day"),
  
  ggplot(Data, aes(x=Sex, fill=FraudFound_P)) + 
    geom_bar(position="dodge") + 
    labs(title="Sex"),
  
  ggplot(Data, aes(x=MaritalStatus, fill=FraudFound_P)) + 
    geom_bar(position="dodge") + 
    labs(title="Marital Status"),
  
  ggplot(Data, aes(x=NumberOfCars, fill=FraudFound_P)) + 
    geom_bar(position="dodge") + 
    labs(title="Number Of Cars"),
  
  ggplot(Data, aes(x=AccidentArea, fill=FraudFound_P)) + 
    geom_bar(position="dodge") + 
    labs(title="Accident Area"),
  
  ggplot(Data, aes(x=DriverRating, fill=FraudFound_P)) + 
    geom_bar(position="dodge") + 
    labs(title="Driver Rating"),
  
  ggplot(Data, aes(x=AgentType, fill=FraudFound_P)) + 
    geom_bar(position="dodge") + 
    labs(title="Agent Type"),
  
  ggplot(Data, aes(x=BasePolicy, fill=FraudFound_P)) + 
    geom_bar(position="dodge") + 
    labs(title="Base Policy")
)

# Arrange the plots in a grid
grid.arrange(
  grobs = plots,
  ncol = 3,
  nrow = 3,
  top = "Countplots of Categorical Variables by Fraud Found",
  widths = c(10, 10, 10),
  heights = c(5, 5, 5)
)

```
```{r}
ggplot(Data, aes(x=FraudFound_P)) + 
    geom_bar(position="dodge") + 
    labs(title="Base Policy")
```

```{r}
table(Data$FraudFound_P)
```

```{r}
# library(ggplot2)
# library(gridExtra)
# df <- Data
# pdf("countplots.pdf", width = 15, height = 10)
# 
# p1 <- ggplot(df, aes(x=Month, fill=FraudFound_P)) + 
#   geom_bar(position="dodge") + 
#   scale_x_discrete(limits=c('Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec')) + 
#   labs(title="Month")
# 
# p2 <- ggplot(df, aes(x=DayOfWeek, fill=FraudFound_P)) + 
#   geom_bar(position="dodge") + 
#   scale_x_discrete(limits=c('Sunday','Monday','Tuesday','Wednesday','Thursday','Friday','Saturday')) + 
#   labs(title="Day")
# 
# p3 <- ggplot(df, aes(x=Sex, fill=FraudFound_P)) + 
#   geom_bar(position="dodge") + 
#   labs(title="Sex")
# 
# p4 <- ggplot(df, aes(x=MaritalStatus, fill=FraudFound_P)) + 
#   geom_bar(position="dodge") + 
#   labs(title="Marital Status")
# 
# p5 <- ggplot(df, aes(x=NumberOfCars, fill=FraudFound_P)) + 
#   geom_bar(position="dodge") + 
#   labs(title="Number Of Cars")
# 
# p6 <- ggplot(df, aes(x=AccidentArea, fill=FraudFound_P)) + 
#   geom_bar(position="dodge") + 
#   labs(title="Accident Area")
# 
# p7 <- ggplot(df, aes(x=DriverRating, fill=FraudFound_P)) + 
#   geom_bar(position="dodge") + 
#   labs(title="Driver Rating")
# 
# p8 <- ggplot(df, aes(x=AgentType, fill=FraudFound_P)) + 
#   geom_bar(position="dodge") + 
#   labs(title="Agent Type")
# 
# p9 <- ggplot(df, aes(x=BasePolicy, fill=FraudFound_P)) + 
#   geom_bar(position="dodge") + 
#   labs(title="Base Policy")
# 
# grid.arrange(p1, p2, p3, p4, p5, p6, p7, p8, p9, ncol=3, nrow=3, top="Countplots of Categorical Variables by Fraud Found")
# 
# dev.off()

```

```{r,warning=FALSE,message=FALSE message=FALSE, r,warning=FALSE}
library(tidyverse)
# find categorical variables
cat_vars <- sapply(Data, class) %>% 
  .[. != "numeric" & . != "integer" & . != "double"] %>% 
  names()

# count unique values in each categorical variable
cat_counts <- lapply(Data[, cat_vars], function(x) length(unique(x)))

# combine variable names and counts into a data frame
cat_summary <- data.frame(
  Variable = names(cat_counts),
  Unique_Count = unlist(cat_counts)
)

# print results
cat_summary
```
```{r}
library(metan)
plot(corr_coef(Data))
```
```{r}
non_collinear_vars(Data,max_vif = 5)
```
```{r}
library(dplyr)
# Data<-Data%>%select(-Year)
```

```{r,warning=FALSE}
library(caret)
# One-Hot Encoding
# Creating dummy variables is converting a categorical variable to as many binary variables as here are categories.
dummies_model <- dummyVars(FraudFound_P ~ ., data=Data)

# Create the dummy variables using predict. The Y variable (Purchase) will not be present in trainData_mat.
trainData_mat <- predict(dummies_model, newdata = Data)

# # Convert to dataframe
Data <- data.frame(trainData_mat)

# # See the structure of the new dataset
str(Data)
```
```{r}
# Normalize numerical variables (excluding target variable)
preprocessParams <- preProcess(Data, method = "BoxCox") #BoxCox: Remove skewness leading to normality. Values must be > 0
```

```{r}
Data <- predict(preprocessParams, newdata = Data)
```
```{r}
# Append the Y variable
Data$FraudFound_P <- y

apply(Data[, 1:10], 2, FUN=function(x){c('min'=min(x), 'max'=max(x))})
```
```{r}
# set.seed(100)
# # options(warn=-1)
# 
# # subsets <- c(1:5, 10, 15, 18)
# 
# ctrl <- rfeControl(functions = rfFuncs,
#                    method = "repeatedcv",
#                    repeats = 2,
#                    verbose = FALSE)
# 
# lmProfile <- rfe(x=Data[, 1:148], 
#                  y=as.factor(Data$FraudFound_P),
#                  # sizes = subsets,
#                  rfeControl = ctrl)
# 
# lmProfile
```

```{r}
ggplot(Data, aes(x=Data$FraudFound_P)) + 
    geom_bar(position="dodge") + 
    labs(title="Agent Type")
```
```{r}
Data$FraudFound_P <- as.factor(Data$FraudFound_P) 
## checking the class distribution of this artificial data set
table(Data$FraudFound_P)
```
SMOTE (Chawla et. al. 2002) is a well-known algorithm to fight this problem. The general idea of this method is to artificially generate new examples of the minority class using the nearest neighbors of these cases. Furthermore, the majority class examples are also under-sampled, leading to a more balanced dataset.
```{r}
library(DMwR)
## now using SMOTE to create a more "balanced problem"
newData <- SMOTE(FraudFound_P ~ ., Data, perc.over = 600,perc.under=100)
table(newData$FraudFound_P)
```

```{r}
# Split data into training, validation, and testing sets
set.seed(123)  # for reproducibility
inTrain <- createDataPartition(Data$FraudFound_P, p = 0.7, list = FALSE)
training <- Data[inTrain, ]
remaining <- Data[-inTrain, ]
inTest <- createDataPartition(remaining$FraudFound_P, p = 0.5, list = FALSE)
validation <- remaining[inTest, ]
testing <- remaining[-inTest, ]
```



```{r}
library(xgboost)

# Define the grid of hyperparameters to search
hyperparams <- expand.grid(
  max_depth = c(1, 2, 3, 4),
  eta = c(0.1, 0.3, 0.5),
  gamma = c(0, 0.2, 0.5),
  colsample_bytree = c(0.5, 0.8),
  min_child_weight = c(1, 3),
  subsample = c(0.5, 0.8),
  nrounds = c(50, 100, 200, 300)
)
```

```{r,warning=FALSE}
# Train and tune the XGBoost model using cross-validation
set.seed(123)
ctrl <- trainControl(method = "repeatedcv",
                     number = 10,
                     verboseIter = TRUE,
                     allowParallel = TRUE, 
                     classProbs = FALSE,
                     search = "random")
model1 <- train(
 factor( FraudFound_P) ~ .,
  data = training,
  trControl = ctrl,
  method = "xgbTree",
  metric = "AUC",
  tuneLength = 2, # number of combinations to try in hyperparameter tuning
  verbose = TRUE,
  objective = "binary:logistic", # specify objective for binary classification
  eval_metric = "auc" # specify evaluation metric for binary classification
)
```
```{r}

# Print the best hyperparameters and model performance
print(model1$bestTune)
print(model1$results)
print(model1$finalModel)

```

```{r}
varImp(model1)
```

```{r}
plot(varImp(model1), main="Variable Importance")
```

```{r}
# Evaluate the model performance on the testing set
library(pROC)

# Make predictions on the validation set
pred_validation <- predict(model1, validation)
```
```{r}

# Compute and print the confusion matrix
confusionMatrix(reference = factor(validation$FraudFound_P), data = pred_validation, mode='everything', positive='1')
```

The confusion matrix shows that the XGBOOST model correctly predicted 2147 true negatives and 40 true positives. However, it incorrectly predicted 113 false negatives and 13 false positives.

The overall accuracy of the model is 0.9455, which means it correctly classified 94.55% of the observations in the testing set. The model has a sensitivity of 0.26144, which means it correctly identified 26.14% of the fraud cases in the testing set. The specificity of the model is 0.99398, which means it correctly identified 99.398% of the non-fraud cases in the testing set.

The precision of the model is 0.75472, which means out of all the cases predicted as fraud, 75.47% were actually fraud cases. The negative predictive value (NPV) of the model is 0.95, which means out of all the cases predicted as non-fraud, 95% were actually non-fraud cases. The NPV indicates the ability of the model to identify true negative cases.

The F1 score of the model is 0.38835, which is the harmonic mean of precision and recall. It is a measure of the tradeoff between precision and recall. In this case, the F1 score indicates that the model has a relatively low performance in terms of identifying fraud cases.

The balanced accuracy of the model is 0.62771, which is the average of sensitivity and specificity. It is a measure of how well the model performs on both positive and negative classes. In this case, the balanced accuracy indicates that the model has a relatively poor performance in terms of identifying fraud cases.


True negatives (TN) are the cases in which the actual class is negative (not fraud) and the model predicts negative (not fraud). In other words, TN represents the number of correctly classified negative cases. In the confusion matrix, TN is located at the top left corner.
```{r}
# Compute and print the AUC
auc <- roc(as.numeric(validation$FraudFound),as.numeric( pred_validation))$auc
cat("AUC:", auc, "\n")

# Plot the ROC curve
plot(roc(as.numeric(validation$FraudFound),as.numeric( pred_validation)), main = "ROC Curve", print.thres = c(0.1, 0.25, 0.5, 0.75, 0.9))


```

```{r}
# Plot the feature importance
xgb.importance(model = model1$finalModel) 

```
```{r,warning=FALSE}
# Define positive class weight
pos_weight <- sum(training$FraudFound_P == 0) / sum(training$FraudFound_P == 1)

# Train the model with class weights
model1 <- train(
  factor(FraudFound_P) ~ .,
  data = training,
  trControl = ctrl,
  method = "xgbTree",
  metric = "AUC",
  tuneLength = 2,
  verbose = TRUE,
  objective = "binary:logistic",
  eval_metric = "auc",
  scale_pos_weight = pos_weight
)

```
```{r}
# Make predictions on the validation set
pred_validation <- predict(model1, validation)
# Compute and print the confusion matrix
confusionMatrix(reference = factor(validation$FraudFound_P), data = pred_validation, mode='everything', positive='1')
```
Using class weights has slightly improved the sensitivity of the model, although at the cost of a lower precision. The new confusion matrix shows that the model correctly identified 22% of the fraudulent cases (sensitivity), while still maintaining a high level of specificity (99.8%). The positive predictive value (precision) has decreased to 89%, meaning that out of all the cases predicted as fraudulent, only 89% are actually fraudulent. The negative predictive value (NPV) has increased to 94.7%, meaning that out of all the cases predicted as non-fraudulent, 94.7% are actually non-fraudulent.
```{r}
# Split data into training, validation, and testing sets
set.seed(123)  # for reproducibility
inTrain <- createDataPartition(newData$FraudFound_P, p = 0.7, list = FALSE)
training <- newData[inTrain, ]
remaining <- newData[-inTrain, ]
inTest <- createDataPartition(remaining$FraudFound_P, p = 0.5, list = FALSE)
validation <- remaining[inTest, ]
testing <- remaining[-inTest, ]
```

```{r,warning=FALSE}
# Define tuning grid
tuneGrid <- expand.grid(nrounds = c(50, 100),
                        max_depth = c(3, 6, 9),
                        eta = c(0.01, 0.1),
                        gamma = 0,
                        colsample_bytree = c(0.5, 0.8),
                        min_child_weight = 1,
                        subsample = c(0.5, 0.8))
# Train and tune the XGBoost model with class weights and hyperparameter tuning
set.seed(123)
ctrl <- trainControl(method = "repeatedcv",
                     number = 10,
                     verboseIter = TRUE,
                     allowParallel = TRUE, 
                     # classProbs = TRUE,
                     search = "random")
model_tuned <- train(
                factor(FraudFound_P) ~ .,
                data = newData,
                trControl = ctrl,
                method = "xgbTree",
                metric = "AUC",
                tuneGrid = tuneGrid,
                verbose = TRUE,
                objective = "binary:logistic",
                eval_metric = "auc"
                # scale_pos_weight = pos_weight
                )
```
```{r}
# Print the best model and its performance
print(model_tuned$bestTune)
print(model_tuned$results)
```

```{r}
# Make predictions on the validation set using the tuned model
pred_validation_tuned <- predict(model_tuned, validation)

# Print the confusion matrix and classification report for the tuned model
confusionMatrix(as.factor(pred_validation_tuned), as.factor(validation$FraudFound_P))

```
Great! The model's performance has significantly improved after using the SMOTE technique. The accuracy has increased to 0.9978, and the precision and recall values are very high for both classes. The confusion matrix shows that the model has made only four misclassifications.

We can conclude that the XGBoost model with SMOTE sampling has performed well on this dataset.


































